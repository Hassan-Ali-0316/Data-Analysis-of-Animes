{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping Titles and HREF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 0\n",
    "limits = '0'\n",
    "pages = 20\n",
    "href = []\n",
    "titles = []\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'}\n",
    "\n",
    "for p in range(pages):\n",
    "\n",
    "    url = 'https://myanimelist.net/topanime.phphttps://myanimelist.net/anime/52991/topanime.php?limit=limits'\n",
    "    page = requests.get(url,headers=headers)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content,'html.parser')\n",
    "        links = soup.find_all('h3', class_='fl-l fs14 fw-b anime_ranking_h3')\n",
    "\n",
    "        for h3 in links:\n",
    "            a_tag = h3.find('a')\n",
    "            if a_tag:\n",
    "                href.append(a_tag.get('href'))\n",
    "                titles.append(a_tag.get_text())\n",
    "\n",
    "    else:\n",
    "        print('Response not 200')\n",
    "        break\n",
    "    \n",
    "    limit +=50\n",
    "    limits = str(limit)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(newSoup):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "description = []\n",
    "rank = []\n",
    "popularity = []\n",
    "episodes = []\n",
    "status = []\n",
    "aired = []\n",
    "premiered = []\n",
    "source = []\n",
    "genres = []\n",
    "rating = []\n",
    "\n",
    "\n",
    "def allPages(href):\n",
    "\n",
    "    for hr in href:\n",
    "        newPage = requests.get(hr,headers=headers)\n",
    "        newSoup = BeautifulSoup(newPage.content,'html.parser')\n",
    "        newSoup.prettify()\n",
    "        sc,desc,r,pop,ep,stat,air,prem,src,gen,rat =  extract_info(newSoup)\n",
    "\n",
    "        \n",
    "        sc = float(sc)\n",
    "        r = r.replace('Ranked#','')\n",
    "        r = float(r)\n",
    "        pop = pop.replace('Popularity#','')\n",
    "        pop = int(pop)\n",
    "        if ep != 'Unknown':\n",
    "            ep = int(ep)\n",
    "        else:\n",
    "            ep = '0'\n",
    "            ep = int(ep)\n",
    "                \n",
    "        if(prem != None):\n",
    "            prem = prem.split(' ',1)[-1]\n",
    "            prem = int(prem)\n",
    "  #      print(sc)\n",
    "  #      print(r)\n",
    "  #      print(pop)\n",
    "  #      print(ep)\n",
    "  #      print(prem)\n",
    "  #      print(desc)\n",
    "  #      print(stat)\n",
    "  #      print(air)\n",
    "  #      print(gen)\n",
    "  #      print(src)\n",
    "  #      print(rat)\n",
    "        scores.append(sc)\n",
    "        description.append(desc)\n",
    "        rank.append(r)\n",
    "        popularity.append(pop)\n",
    "        episodes.append(ep)\n",
    "        status.append(stat)\n",
    "        aired.append(air)\n",
    "        premiered.append(prem)\n",
    "        source.append(src)\n",
    "        genres.append(gen)\n",
    "        rating.append(rat)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(newSoup):\n",
    "    description = None\n",
    "    rank = None\n",
    "    popularity = None\n",
    "    episodes = None\n",
    "    status = None\n",
    "    aired = None\n",
    "    premiered = None\n",
    "    source = None\n",
    "    genres = None\n",
    "    rating = None\n",
    "\n",
    "    # Extract score if present\n",
    "    score = newSoup.find('div', class_='score-label')\n",
    "    score_text = score.get_text(strip=True) if score else \"Score not found\"\n",
    "\n",
    "    # Extract rank if present\n",
    "    rankt = newSoup.find('span', class_='numbers ranked')\n",
    "    rank = rankt.get_text(strip=True) if rankt else \"Rank not found\"\n",
    "\n",
    "    # Extract popularity if present\n",
    "    popularityp = newSoup.find('span', class_='numbers popularity')\n",
    "    popularity = popularityp.get_text(strip=True) if popularityp else \"Popularity not found\"\n",
    "\n",
    "    # Extract description if present\n",
    "    descriptiond = newSoup.find('p', itemprop='description')\n",
    "    description = descriptiond.get_text(strip=True) if descriptiond else \"Description not found\"\n",
    "\n",
    "    allDivs = newSoup.find_all('div', class_='spaceit_pad')\n",
    "\n",
    "    for div in allDivs:\n",
    "        div_text = div.get_text(strip=True)\n",
    "        \n",
    "        if 'Episodes' in div_text:\n",
    "            episodes = div_text.replace('Episodes:', '').strip()\n",
    "        \n",
    "        elif 'Status' in div_text:\n",
    "            status = div_text.replace('Status:', '').strip()\n",
    "        \n",
    "        elif 'Aired' in div_text:\n",
    "            aired = div_text.replace('Aired:', '').strip()\n",
    "    \n",
    "        elif 'Premiered' in div_text:\n",
    "            premiered = div_text.replace('Premiered:', '').strip()\n",
    "        \n",
    "        elif 'Source' in div_text:\n",
    "            source = div_text.replace('Source:', '').strip()\n",
    "        \n",
    "        elif 'Genres' in div_text:\n",
    "            # Extract genres from <a> tags within the div\n",
    "            genres = [a_tag.get_text(strip=True) for a_tag in div.find_all('a')]\n",
    "            genres = ', '.join(genres)\n",
    "        \n",
    "        elif 'Rating' in div_text:\n",
    "            rating = div_text.replace('Rating:', '').strip()\n",
    "    \n",
    "    return score_text,description, rank, popularity, episodes, status, aired, premiered, source, genres, rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))\n",
    "allPages(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a pandas dataframe and then stored in .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 13)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Rank':rank,'Link':href,'Title':titles,'Score':scores,'Description':description,'Year Premiered':premiered,'Episodes':episodes,'Popularity':popularity,'Genre':genres,'Status':status,'Source':source,'Rating':rating,'Aired':aired})\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis of Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     neg    neu    pos  compound\n",
      "0  0.055  0.796  0.149    0.9432\n",
      "1  0.175  0.755  0.070   -0.9690\n",
      "2  0.138  0.719  0.143   -0.0015\n",
      "3  0.131  0.755  0.114   -0.5930\n",
      "4  0.107  0.856  0.037   -0.8221\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "sentimentScore = []\n",
    "for desc in description:\n",
    "    sentimentScore.append(sia.polarity_scores(desc))\n",
    "\n",
    "df2 = pd.DataFrame(sentimentScore)\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging both dataframes into one and then saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use it as a column for merging\n",
    "df1_reset = df.reset_index()\n",
    "df2_reset = df2.reset_index()\n",
    "\n",
    "# Merge DataFrames on the index column\n",
    "final = pd.merge(df1_reset, df2_reset, on='index', how='left')\n",
    "\n",
    "# Drop the extra index column if desired\n",
    "final = final.drop(columns='index')\n",
    "\n",
    "# Save to CSV\n",
    "final.to_csv('Anime_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
